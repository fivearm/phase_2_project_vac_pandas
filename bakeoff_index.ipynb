{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression Bakeoff"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Paul Hollywood gif](https://media.giphy.com/media/OjrcZp4fXMHBryKoXZ/giphy.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inferential vs. Predictive\n",
    "You should think of this primarily as a project in **inferential** statistics. That means:\n",
    "- focusing on trying to satisfy the assumptions of linear regression;\n",
    "- using all your records to build models;\n",
    "- aiming for understanding how features influence sales prices.\n",
    "\n",
    "But we also invite you to a level-up: a friendly competition among the teams. And here the goal is **predictive**. That means:\n",
    "- maximizing $R^2$;\n",
    "- utilizing train-test splits;\n",
    "- utilizing validation sets (or cross-validation).\n",
    "Weâ€™ll have SOME UNLABELED TEST DATA FOR YOU TO PLUG INTO YOUR MODELS.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Like a Kaggle competition, you are provided with the following training data representing 3/4 of the data set.  \n",
    "It is split into **predictive features** (X_train) and **target variable** (y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd \n",
    "from scipy import stats\n",
    "import warnings\n",
    "\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, PolynomialFeatures\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.dummy import DummyRegressor\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.model_selection import cross_validate\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import time\n",
    "start_time = time.time()\n",
    "%matplotlib inline\n",
    "\n",
    "X_train = pd.read_csv('bakeoff_data/Xtrain.csv')\n",
    "y_train = pd.read_csv('bakeoff_data/ytrain.csv')\n",
    "X_test = pd.read_csv('bakeoff_data/Xtest.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5400, 19)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(16197, 19)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(16197, 1)\n"
     ]
    }
   ],
   "source": [
    "print(y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>bedrooms</th>\n",
       "      <th>bathrooms</th>\n",
       "      <th>sqft_living</th>\n",
       "      <th>sqft_lot</th>\n",
       "      <th>floors</th>\n",
       "      <th>waterfront</th>\n",
       "      <th>view</th>\n",
       "      <th>condition</th>\n",
       "      <th>grade</th>\n",
       "      <th>sqft_above</th>\n",
       "      <th>sqft_basement</th>\n",
       "      <th>yr_built</th>\n",
       "      <th>yr_renovated</th>\n",
       "      <th>zipcode</th>\n",
       "      <th>lat</th>\n",
       "      <th>long</th>\n",
       "      <th>sqft_living15</th>\n",
       "      <th>sqft_lot15</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3/4/2015</td>\n",
       "      <td>3</td>\n",
       "      <td>2.50</td>\n",
       "      <td>1880</td>\n",
       "      <td>4499</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3</td>\n",
       "      <td>8</td>\n",
       "      <td>1880</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1993</td>\n",
       "      <td>0.0</td>\n",
       "      <td>98029</td>\n",
       "      <td>47.5664</td>\n",
       "      <td>-121.999</td>\n",
       "      <td>2130</td>\n",
       "      <td>5114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10/7/2014</td>\n",
       "      <td>3</td>\n",
       "      <td>2.50</td>\n",
       "      <td>2020</td>\n",
       "      <td>6564</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>1310</td>\n",
       "      <td>710.0</td>\n",
       "      <td>1994</td>\n",
       "      <td>0.0</td>\n",
       "      <td>98042</td>\n",
       "      <td>47.3545</td>\n",
       "      <td>-122.158</td>\n",
       "      <td>1710</td>\n",
       "      <td>5151</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1/16/2015</td>\n",
       "      <td>5</td>\n",
       "      <td>4.00</td>\n",
       "      <td>4720</td>\n",
       "      <td>493534</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5</td>\n",
       "      <td>9</td>\n",
       "      <td>3960</td>\n",
       "      <td>760.0</td>\n",
       "      <td>1975</td>\n",
       "      <td>0.0</td>\n",
       "      <td>98027</td>\n",
       "      <td>47.4536</td>\n",
       "      <td>-122.009</td>\n",
       "      <td>2160</td>\n",
       "      <td>219542</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3/30/2015</td>\n",
       "      <td>2</td>\n",
       "      <td>2.00</td>\n",
       "      <td>1430</td>\n",
       "      <td>3880</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4</td>\n",
       "      <td>7</td>\n",
       "      <td>1430</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1949</td>\n",
       "      <td>0.0</td>\n",
       "      <td>98117</td>\n",
       "      <td>47.6844</td>\n",
       "      <td>-122.392</td>\n",
       "      <td>1430</td>\n",
       "      <td>3880</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10/14/2014</td>\n",
       "      <td>3</td>\n",
       "      <td>2.25</td>\n",
       "      <td>2270</td>\n",
       "      <td>32112</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4</td>\n",
       "      <td>8</td>\n",
       "      <td>1740</td>\n",
       "      <td>530.0</td>\n",
       "      <td>1980</td>\n",
       "      <td>0.0</td>\n",
       "      <td>98042</td>\n",
       "      <td>47.3451</td>\n",
       "      <td>-122.094</td>\n",
       "      <td>2310</td>\n",
       "      <td>41606</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         date  bedrooms  bathrooms  sqft_living  sqft_lot  floors  waterfront  \\\n",
       "0    3/4/2015         3       2.50         1880      4499     2.0         0.0   \n",
       "1   10/7/2014         3       2.50         2020      6564     1.0         0.0   \n",
       "2   1/16/2015         5       4.00         4720    493534     2.0         0.0   \n",
       "3   3/30/2015         2       2.00         1430      3880     1.0         0.0   \n",
       "4  10/14/2014         3       2.25         2270     32112     1.0         0.0   \n",
       "\n",
       "   view  condition  grade  sqft_above sqft_basement  yr_built  yr_renovated  \\\n",
       "0   0.0          3      8        1880           0.0      1993           0.0   \n",
       "1   0.0          3      7        1310         710.0      1994           0.0   \n",
       "2   0.0          5      9        3960         760.0      1975           0.0   \n",
       "3   0.0          4      7        1430           0.0      1949           0.0   \n",
       "4   0.0          4      8        1740         530.0      1980           0.0   \n",
       "\n",
       "   zipcode      lat     long  sqft_living15  sqft_lot15  \n",
       "0    98029  47.5664 -121.999           2130        5114  \n",
       "1    98042  47.3545 -122.158           1710        5151  \n",
       "2    98027  47.4536 -122.009           2160      219542  \n",
       "3    98117  47.6844 -122.392           1430        3880  \n",
       "4    98042  47.3451 -122.094           2310       41606  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "date                0\n",
       "bedrooms            0\n",
       "bathrooms           0\n",
       "sqft_living         0\n",
       "sqft_lot            0\n",
       "floors              0\n",
       "waterfront       1756\n",
       "view               49\n",
       "condition           0\n",
       "grade               0\n",
       "sqft_above          0\n",
       "sqft_basement       0\n",
       "yr_built            0\n",
       "yr_renovated     2879\n",
       "zipcode             0\n",
       "lat                 0\n",
       "long                0\n",
       "sqft_living15       0\n",
       "sqft_lot15          0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Cleaning X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set Nan values of 'waterfront' and 'year_renovated' columns to 0\n",
    "X_train.loc[X_train.waterfront.isna()==True, 'waterfront'] = 0\n",
    "X_train.loc[X_train.yr_renovated.isna()==True, 'yr_renovated'] = 0\n",
    "\n",
    "#Set all sqft_basement values of '?' to 0, then convert to floats.\n",
    "X_train.loc[X_train.sqft_basement=='?', 'sqft_basement'] = 0\n",
    "X_train.sqft_basement = X_train.sqft_basement.astype(float)\n",
    "\n",
    "# Convert 'date' to a datetime object and use these to create a 'year' column\n",
    "X_train['date'] = pd.to_datetime(X_train['date'])\n",
    "X_train['year'] = X_train['date'].apply(lambda date: date.year)\n",
    "\n",
    "# Create an 'age' column to specify how old a house was at sale\n",
    "X_train['age'] = X_train['year'] - X_train['yr_built']\n",
    "\n",
    "#Drop unnecessary 'id', 'yr_built', 'year', and 'date' columns\n",
    "cols_to_drop = ['yr_built', 'year', 'date']\n",
    "X_train.drop(cols_to_drop, axis=1, inplace=True)\n",
    "\n",
    "#Drop rows that contain null values in the 'view' column\n",
    "X_train.loc[X_train.view.isna()==True, 'view'] = X_train.view.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Cleaning X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set Nan values of 'waterfront' and 'year_renovated' columns to 0\n",
    "X_test.loc[X_test.waterfront.isna()==True, 'waterfront'] = 0\n",
    "X_test.loc[X_test.yr_renovated.isna()==True, 'yr_renovated'] = 0\n",
    "\n",
    "#Set all sqft_basement values of '?' to 0, then convert to floats.\n",
    "X_test.loc[X_test.sqft_basement=='?', 'sqft_basement'] = 0\n",
    "X_test.sqft_basement = X_test.sqft_basement.astype(float)\n",
    "\n",
    "# Convert 'date' to a datetime object and use these to create a 'year' column\n",
    "X_test['date'] = pd.to_datetime(X_test['date'])\n",
    "X_test['year'] = X_test['date'].apply(lambda date: date.year)\n",
    "\n",
    "# Create an 'age' column to specify how old a house was at sale\n",
    "X_test['age'] = X_test['year'] - X_test['yr_built']\n",
    "\n",
    "#Drop unnecessary 'id', 'yr_built', 'year', and 'date' columns\n",
    "cols_to_drop = ['yr_built', 'year', 'date']\n",
    "X_test.drop(cols_to_drop, axis=1, inplace=True)\n",
    "\n",
    "#Drop rows that contain null values in the 'view' column\n",
    "X_test.loc[X_test.view.isna()==True, 'view'] = X_test.view.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5400, 18)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add dummy zipcodes to X_train and X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adds dummy zipcode columns\n",
    "X_test = X_test.join(pd.get_dummies(X_test['zipcode'], prefix = 'x', drop_first = True))\n",
    "X_test.drop('zipcode', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.join(pd.get_dummies(X_train['zipcode'], prefix = 'x', drop_first = True))\n",
    "X_train.drop('zipcode', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 16197 entries, 0 to 16196\n",
      "Data columns (total 1 columns):\n",
      " #   Column  Non-Null Count  Dtype  \n",
      "---  ------  --------------  -----  \n",
      " 0   price   16197 non-null  float64\n",
      "dtypes: float64(1)\n",
      "memory usage: 126.7 KB\n"
     ]
    }
   ],
   "source": [
    "y_train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 5400 entries, 0 to 5399\n",
      "Data columns (total 86 columns):\n",
      " #   Column         Non-Null Count  Dtype  \n",
      "---  ------         --------------  -----  \n",
      " 0   bedrooms       5400 non-null   int64  \n",
      " 1   bathrooms      5400 non-null   float64\n",
      " 2   sqft_living    5400 non-null   int64  \n",
      " 3   sqft_lot       5400 non-null   int64  \n",
      " 4   floors         5400 non-null   float64\n",
      " 5   waterfront     5400 non-null   float64\n",
      " 6   view           5400 non-null   float64\n",
      " 7   condition      5400 non-null   int64  \n",
      " 8   grade          5400 non-null   int64  \n",
      " 9   sqft_above     5400 non-null   int64  \n",
      " 10  sqft_basement  5400 non-null   float64\n",
      " 11  yr_renovated   5400 non-null   float64\n",
      " 12  lat            5400 non-null   float64\n",
      " 13  long           5400 non-null   float64\n",
      " 14  sqft_living15  5400 non-null   int64  \n",
      " 15  sqft_lot15     5400 non-null   int64  \n",
      " 16  age            5400 non-null   int64  \n",
      " 17  x_98002        5400 non-null   uint8  \n",
      " 18  x_98003        5400 non-null   uint8  \n",
      " 19  x_98004        5400 non-null   uint8  \n",
      " 20  x_98005        5400 non-null   uint8  \n",
      " 21  x_98006        5400 non-null   uint8  \n",
      " 22  x_98007        5400 non-null   uint8  \n",
      " 23  x_98008        5400 non-null   uint8  \n",
      " 24  x_98010        5400 non-null   uint8  \n",
      " 25  x_98011        5400 non-null   uint8  \n",
      " 26  x_98014        5400 non-null   uint8  \n",
      " 27  x_98019        5400 non-null   uint8  \n",
      " 28  x_98022        5400 non-null   uint8  \n",
      " 29  x_98023        5400 non-null   uint8  \n",
      " 30  x_98024        5400 non-null   uint8  \n",
      " 31  x_98027        5400 non-null   uint8  \n",
      " 32  x_98028        5400 non-null   uint8  \n",
      " 33  x_98029        5400 non-null   uint8  \n",
      " 34  x_98030        5400 non-null   uint8  \n",
      " 35  x_98031        5400 non-null   uint8  \n",
      " 36  x_98032        5400 non-null   uint8  \n",
      " 37  x_98033        5400 non-null   uint8  \n",
      " 38  x_98034        5400 non-null   uint8  \n",
      " 39  x_98038        5400 non-null   uint8  \n",
      " 40  x_98039        5400 non-null   uint8  \n",
      " 41  x_98040        5400 non-null   uint8  \n",
      " 42  x_98042        5400 non-null   uint8  \n",
      " 43  x_98045        5400 non-null   uint8  \n",
      " 44  x_98052        5400 non-null   uint8  \n",
      " 45  x_98053        5400 non-null   uint8  \n",
      " 46  x_98055        5400 non-null   uint8  \n",
      " 47  x_98056        5400 non-null   uint8  \n",
      " 48  x_98058        5400 non-null   uint8  \n",
      " 49  x_98059        5400 non-null   uint8  \n",
      " 50  x_98065        5400 non-null   uint8  \n",
      " 51  x_98070        5400 non-null   uint8  \n",
      " 52  x_98072        5400 non-null   uint8  \n",
      " 53  x_98074        5400 non-null   uint8  \n",
      " 54  x_98075        5400 non-null   uint8  \n",
      " 55  x_98077        5400 non-null   uint8  \n",
      " 56  x_98092        5400 non-null   uint8  \n",
      " 57  x_98102        5400 non-null   uint8  \n",
      " 58  x_98103        5400 non-null   uint8  \n",
      " 59  x_98105        5400 non-null   uint8  \n",
      " 60  x_98106        5400 non-null   uint8  \n",
      " 61  x_98107        5400 non-null   uint8  \n",
      " 62  x_98108        5400 non-null   uint8  \n",
      " 63  x_98109        5400 non-null   uint8  \n",
      " 64  x_98112        5400 non-null   uint8  \n",
      " 65  x_98115        5400 non-null   uint8  \n",
      " 66  x_98116        5400 non-null   uint8  \n",
      " 67  x_98117        5400 non-null   uint8  \n",
      " 68  x_98118        5400 non-null   uint8  \n",
      " 69  x_98119        5400 non-null   uint8  \n",
      " 70  x_98122        5400 non-null   uint8  \n",
      " 71  x_98125        5400 non-null   uint8  \n",
      " 72  x_98126        5400 non-null   uint8  \n",
      " 73  x_98133        5400 non-null   uint8  \n",
      " 74  x_98136        5400 non-null   uint8  \n",
      " 75  x_98144        5400 non-null   uint8  \n",
      " 76  x_98146        5400 non-null   uint8  \n",
      " 77  x_98148        5400 non-null   uint8  \n",
      " 78  x_98155        5400 non-null   uint8  \n",
      " 79  x_98166        5400 non-null   uint8  \n",
      " 80  x_98168        5400 non-null   uint8  \n",
      " 81  x_98177        5400 non-null   uint8  \n",
      " 82  x_98178        5400 non-null   uint8  \n",
      " 83  x_98188        5400 non-null   uint8  \n",
      " 84  x_98198        5400 non-null   uint8  \n",
      " 85  x_98199        5400 non-null   uint8  \n",
      "dtypes: float64(8), int64(9), uint8(69)\n",
      "memory usage: 1.1 MB\n"
     ]
    }
   ],
   "source": [
    "X_test.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Drop columns in X_train and X_test.  Log transform y_train."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.drop(['sqft_basement', 'bedrooms', 'sqft_above'], axis = 1)\n",
    "X_test = X_test.drop(['sqft_basement', 'bedrooms', 'sqft_above'], axis = 1)\n",
    "y_train = np.log(y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the model and determine R2 score. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.87538759994488"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialize an empty regression model\n",
    "lr = LinearRegression()\n",
    "# Fits the model to our training dataset\n",
    "lr.fit(X_train, y_train)\n",
    "# Predict price with the trained model\n",
    "pred_lr = lr.predict(X_test)\n",
    "# Get the coefficient of determination for training and test data\n",
    "train_score_lr = lr.score(X_train, y_train)\n",
    "# Take a peak at model coef\n",
    "lr.coef_[0]\n",
    "# Baseline housing cost without features\n",
    "lr.intercept_\n",
    "# Normalizing our price to get an RMSE\n",
    "train_score_lr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Statsmodel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>OLS Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>          <td>price</td>      <th>  R-squared:         </th> <td>   0.875</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.875</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>   1364.</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>             <td>Thu, 15 Jul 2021</td> <th>  Prob (F-statistic):</th>  <td>  0.00</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                 <td>18:46:52</td>     <th>  Log-Likelihood:    </th> <td>  4246.4</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations:</th>      <td> 16197</td>      <th>  AIC:               </th> <td>  -8325.</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Residuals:</th>          <td> 16113</td>      <th>  BIC:               </th> <td>  -7679.</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Model:</th>              <td>    83</td>      <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "        <td></td>           <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>const</th>         <td>  -58.9848</td> <td>    8.146</td> <td>   -7.241</td> <td> 0.000</td> <td>  -74.951</td> <td>  -43.018</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>bathrooms</th>     <td>    0.0362</td> <td>    0.003</td> <td>   10.478</td> <td> 0.000</td> <td>    0.029</td> <td>    0.043</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>sqft_living</th>   <td>    0.0002</td> <td> 3.47e-06</td> <td>   49.098</td> <td> 0.000</td> <td>    0.000</td> <td>    0.000</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>sqft_lot</th>      <td> 6.026e-07</td> <td> 5.17e-08</td> <td>   11.645</td> <td> 0.000</td> <td> 5.01e-07</td> <td> 7.04e-07</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>floors</th>        <td>   -0.0012</td> <td>    0.004</td> <td>   -0.323</td> <td> 0.746</td> <td>   -0.009</td> <td>    0.006</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>waterfront</th>    <td>    0.4896</td> <td>    0.020</td> <td>   25.008</td> <td> 0.000</td> <td>    0.451</td> <td>    0.528</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>view</th>          <td>    0.0545</td> <td>    0.002</td> <td>   23.597</td> <td> 0.000</td> <td>    0.050</td> <td>    0.059</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>condition</th>     <td>    0.0523</td> <td>    0.003</td> <td>   20.356</td> <td> 0.000</td> <td>    0.047</td> <td>    0.057</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>grade</th>         <td>    0.1001</td> <td>    0.002</td> <td>   41.884</td> <td> 0.000</td> <td>    0.095</td> <td>    0.105</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>yr_renovated</th>  <td> 3.853e-05</td> <td> 4.33e-06</td> <td>    8.895</td> <td> 0.000</td> <td>    3e-05</td> <td>  4.7e-05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>lat</th>           <td>    0.5708</td> <td>    0.085</td> <td>    6.733</td> <td> 0.000</td> <td>    0.405</td> <td>    0.737</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>long</th>          <td>   -0.3514</td> <td>    0.060</td> <td>   -5.855</td> <td> 0.000</td> <td>   -0.469</td> <td>   -0.234</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>sqft_living15</th> <td> 9.477e-05</td> <td> 3.88e-06</td> <td>   24.441</td> <td> 0.000</td> <td> 8.72e-05</td> <td>    0.000</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>sqft_lot15</th>    <td> 5.398e-08</td> <td> 8.18e-08</td> <td>    0.659</td> <td> 0.510</td> <td>-1.06e-07</td> <td> 2.14e-07</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>age</th>           <td>    0.0007</td> <td> 8.56e-05</td> <td>    7.998</td> <td> 0.000</td> <td>    0.001</td> <td>    0.001</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x_98002</th>       <td>   -0.0127</td> <td>    0.020</td> <td>   -0.642</td> <td> 0.521</td> <td>   -0.051</td> <td>    0.026</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x_98003</th>       <td>   -0.0022</td> <td>    0.017</td> <td>   -0.128</td> <td> 0.898</td> <td>   -0.036</td> <td>    0.032</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x_98004</th>       <td>    0.9006</td> <td>    0.031</td> <td>   28.600</td> <td> 0.000</td> <td>    0.839</td> <td>    0.962</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x_98005</th>       <td>    0.5637</td> <td>    0.034</td> <td>   16.769</td> <td> 0.000</td> <td>    0.498</td> <td>    0.630</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x_98006</th>       <td>    0.4908</td> <td>    0.028</td> <td>   17.777</td> <td> 0.000</td> <td>    0.437</td> <td>    0.545</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x_98007</th>       <td>    0.5024</td> <td>    0.035</td> <td>   14.419</td> <td> 0.000</td> <td>    0.434</td> <td>    0.571</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x_98008</th>       <td>    0.5063</td> <td>    0.033</td> <td>   15.445</td> <td> 0.000</td> <td>    0.442</td> <td>    0.571</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x_98010</th>       <td>    0.3148</td> <td>    0.029</td> <td>   10.889</td> <td> 0.000</td> <td>    0.258</td> <td>    0.371</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x_98011</th>       <td>    0.2054</td> <td>    0.043</td> <td>    4.790</td> <td> 0.000</td> <td>    0.121</td> <td>    0.289</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x_98014</th>       <td>    0.2417</td> <td>    0.047</td> <td>    5.180</td> <td> 0.000</td> <td>    0.150</td> <td>    0.333</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x_98019</th>       <td>    0.1826</td> <td>    0.046</td> <td>    3.946</td> <td> 0.000</td> <td>    0.092</td> <td>    0.273</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x_98022</th>       <td>    0.1932</td> <td>    0.026</td> <td>    7.485</td> <td> 0.000</td> <td>    0.143</td> <td>    0.244</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x_98023</th>       <td>   -0.0757</td> <td>    0.016</td> <td>   -4.766</td> <td> 0.000</td> <td>   -0.107</td> <td>   -0.045</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x_98024</th>       <td>    0.4017</td> <td>    0.041</td> <td>    9.843</td> <td> 0.000</td> <td>    0.322</td> <td>    0.482</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x_98027</th>       <td>    0.4306</td> <td>    0.028</td> <td>   15.411</td> <td> 0.000</td> <td>    0.376</td> <td>    0.485</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x_98028</th>       <td>    0.1506</td> <td>    0.042</td> <td>    3.611</td> <td> 0.000</td> <td>    0.069</td> <td>    0.232</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x_98029</th>       <td>    0.5300</td> <td>    0.032</td> <td>   16.553</td> <td> 0.000</td> <td>    0.467</td> <td>    0.593</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x_98030</th>       <td>    0.0470</td> <td>    0.019</td> <td>    2.421</td> <td> 0.015</td> <td>    0.009</td> <td>    0.085</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x_98031</th>       <td>    0.0383</td> <td>    0.020</td> <td>    1.938</td> <td> 0.053</td> <td>   -0.000</td> <td>    0.077</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x_98032</th>       <td>   -0.0864</td> <td>    0.024</td> <td>   -3.659</td> <td> 0.000</td> <td>   -0.133</td> <td>   -0.040</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x_98033</th>       <td>    0.5801</td> <td>    0.036</td> <td>   16.217</td> <td> 0.000</td> <td>    0.510</td> <td>    0.650</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x_98034</th>       <td>    0.3084</td> <td>    0.038</td> <td>    8.057</td> <td> 0.000</td> <td>    0.233</td> <td>    0.383</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x_98038</th>       <td>    0.2154</td> <td>    0.021</td> <td>   10.110</td> <td> 0.000</td> <td>    0.174</td> <td>    0.257</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x_98039</th>       <td>    1.0385</td> <td>    0.042</td> <td>   24.803</td> <td> 0.000</td> <td>    0.956</td> <td>    1.121</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x_98040</th>       <td>    0.6956</td> <td>    0.028</td> <td>   24.966</td> <td> 0.000</td> <td>    0.641</td> <td>    0.750</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x_98042</th>       <td>    0.0777</td> <td>    0.018</td> <td>    4.267</td> <td> 0.000</td> <td>    0.042</td> <td>    0.113</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x_98045</th>       <td>    0.4060</td> <td>    0.039</td> <td>   10.441</td> <td> 0.000</td> <td>    0.330</td> <td>    0.482</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x_98052</th>       <td>    0.4583</td> <td>    0.036</td> <td>   12.607</td> <td> 0.000</td> <td>    0.387</td> <td>    0.530</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x_98053</th>       <td>    0.4510</td> <td>    0.039</td> <td>   11.582</td> <td> 0.000</td> <td>    0.375</td> <td>    0.527</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x_98055</th>       <td>    0.0617</td> <td>    0.022</td> <td>    2.825</td> <td> 0.005</td> <td>    0.019</td> <td>    0.105</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x_98056</th>       <td>    0.2232</td> <td>    0.024</td> <td>    9.297</td> <td> 0.000</td> <td>    0.176</td> <td>    0.270</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x_98058</th>       <td>    0.1095</td> <td>    0.021</td> <td>    5.241</td> <td> 0.000</td> <td>    0.069</td> <td>    0.150</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x_98059</th>       <td>    0.2757</td> <td>    0.024</td> <td>   11.702</td> <td> 0.000</td> <td>    0.230</td> <td>    0.322</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x_98065</th>       <td>    0.4066</td> <td>    0.036</td> <td>   11.292</td> <td> 0.000</td> <td>    0.336</td> <td>    0.477</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x_98070</th>       <td>    0.2039</td> <td>    0.028</td> <td>    7.277</td> <td> 0.000</td> <td>    0.149</td> <td>    0.259</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x_98072</th>       <td>    0.2707</td> <td>    0.043</td> <td>    6.359</td> <td> 0.000</td> <td>    0.187</td> <td>    0.354</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x_98074</th>       <td>    0.4396</td> <td>    0.034</td> <td>   12.762</td> <td> 0.000</td> <td>    0.372</td> <td>    0.507</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x_98075</th>       <td>    0.4619</td> <td>    0.033</td> <td>   14.008</td> <td> 0.000</td> <td>    0.397</td> <td>    0.527</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x_98077</th>       <td>    0.2543</td> <td>    0.044</td> <td>    5.747</td> <td> 0.000</td> <td>    0.168</td> <td>    0.341</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x_98092</th>       <td>    0.0537</td> <td>    0.018</td> <td>    3.061</td> <td> 0.002</td> <td>    0.019</td> <td>    0.088</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x_98102</th>       <td>    0.6748</td> <td>    0.036</td> <td>   18.503</td> <td> 0.000</td> <td>    0.603</td> <td>    0.746</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x_98103</th>       <td>    0.5292</td> <td>    0.035</td> <td>   15.300</td> <td> 0.000</td> <td>    0.461</td> <td>    0.597</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x_98105</th>       <td>    0.6784</td> <td>    0.035</td> <td>   19.145</td> <td> 0.000</td> <td>    0.609</td> <td>    0.748</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x_98106</th>       <td>    0.1394</td> <td>    0.026</td> <td>    5.439</td> <td> 0.000</td> <td>    0.089</td> <td>    0.190</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x_98107</th>       <td>    0.5354</td> <td>    0.036</td> <td>   14.968</td> <td> 0.000</td> <td>    0.465</td> <td>    0.605</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x_98108</th>       <td>    0.1619</td> <td>    0.028</td> <td>    5.776</td> <td> 0.000</td> <td>    0.107</td> <td>    0.217</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x_98109</th>       <td>    0.7025</td> <td>    0.037</td> <td>   18.945</td> <td> 0.000</td> <td>    0.630</td> <td>    0.775</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x_98112</th>       <td>    0.7919</td> <td>    0.033</td> <td>   24.361</td> <td> 0.000</td> <td>    0.728</td> <td>    0.856</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x_98115</th>       <td>    0.5425</td> <td>    0.035</td> <td>   15.424</td> <td> 0.000</td> <td>    0.474</td> <td>    0.611</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x_98116</th>       <td>    0.5213</td> <td>    0.029</td> <td>   18.192</td> <td> 0.000</td> <td>    0.465</td> <td>    0.577</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x_98117</th>       <td>    0.5089</td> <td>    0.036</td> <td>   14.280</td> <td> 0.000</td> <td>    0.439</td> <td>    0.579</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x_98118</th>       <td>    0.2772</td> <td>    0.025</td> <td>   11.051</td> <td> 0.000</td> <td>    0.228</td> <td>    0.326</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x_98119</th>       <td>    0.7055</td> <td>    0.035</td> <td>   20.382</td> <td> 0.000</td> <td>    0.638</td> <td>    0.773</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x_98122</th>       <td>    0.5581</td> <td>    0.031</td> <td>   18.006</td> <td> 0.000</td> <td>    0.497</td> <td>    0.619</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x_98125</th>       <td>    0.2929</td> <td>    0.038</td> <td>    7.717</td> <td> 0.000</td> <td>    0.219</td> <td>    0.367</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x_98126</th>       <td>    0.3181</td> <td>    0.026</td> <td>   12.019</td> <td> 0.000</td> <td>    0.266</td> <td>    0.370</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x_98133</th>       <td>    0.1580</td> <td>    0.039</td> <td>    4.032</td> <td> 0.000</td> <td>    0.081</td> <td>    0.235</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x_98136</th>       <td>    0.4633</td> <td>    0.027</td> <td>   17.111</td> <td> 0.000</td> <td>    0.410</td> <td>    0.516</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x_98144</th>       <td>    0.4497</td> <td>    0.029</td> <td>   15.597</td> <td> 0.000</td> <td>    0.393</td> <td>    0.506</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x_98146</th>       <td>    0.1122</td> <td>    0.024</td> <td>    4.614</td> <td> 0.000</td> <td>    0.065</td> <td>    0.160</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x_98148</th>       <td>    0.0511</td> <td>    0.032</td> <td>    1.588</td> <td> 0.112</td> <td>   -0.012</td> <td>    0.114</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x_98155</th>       <td>    0.1435</td> <td>    0.041</td> <td>    3.517</td> <td> 0.000</td> <td>    0.064</td> <td>    0.223</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x_98166</th>       <td>    0.1803</td> <td>    0.022</td> <td>    8.200</td> <td> 0.000</td> <td>    0.137</td> <td>    0.223</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x_98168</th>       <td>   -0.0601</td> <td>    0.023</td> <td>   -2.581</td> <td> 0.010</td> <td>   -0.106</td> <td>   -0.014</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x_98177</th>       <td>    0.2821</td> <td>    0.041</td> <td>    6.861</td> <td> 0.000</td> <td>    0.201</td> <td>    0.363</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x_98178</th>       <td>    0.0183</td> <td>    0.024</td> <td>    0.763</td> <td> 0.446</td> <td>   -0.029</td> <td>    0.065</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x_98188</th>       <td>    0.0064</td> <td>    0.024</td> <td>    0.260</td> <td> 0.795</td> <td>   -0.042</td> <td>    0.054</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x_98198</th>       <td>   -0.0338</td> <td>    0.019</td> <td>   -1.792</td> <td> 0.073</td> <td>   -0.071</td> <td>    0.003</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x_98199</th>       <td>    0.5655</td> <td>    0.034</td> <td>   16.742</td> <td> 0.000</td> <td>    0.499</td> <td>    0.632</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td>1368.515</td> <th>  Durbin-Watson:     </th> <td>   2.015</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th>  <td> 0.000</td>  <th>  Jarque-Bera (JB):  </th> <td>6257.953</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>           <td>-0.297</td>  <th>  Prob(JB):          </th> <td>    0.00</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>       <td> 5.987</td>  <th>  Cond. No.          </th> <td>2.77e+08</td>\n",
       "</tr>\n",
       "</table><br/><br/>Notes:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.<br/>[2] The condition number is large, 2.77e+08. This might indicate that there are<br/>strong multicollinearity or other numerical problems."
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                            OLS Regression Results                            \n",
       "==============================================================================\n",
       "Dep. Variable:                  price   R-squared:                       0.875\n",
       "Model:                            OLS   Adj. R-squared:                  0.875\n",
       "Method:                 Least Squares   F-statistic:                     1364.\n",
       "Date:                Thu, 15 Jul 2021   Prob (F-statistic):               0.00\n",
       "Time:                        18:46:52   Log-Likelihood:                 4246.4\n",
       "No. Observations:               16197   AIC:                            -8325.\n",
       "Df Residuals:                   16113   BIC:                            -7679.\n",
       "Df Model:                          83                                         \n",
       "Covariance Type:            nonrobust                                         \n",
       "=================================================================================\n",
       "                    coef    std err          t      P>|t|      [0.025      0.975]\n",
       "---------------------------------------------------------------------------------\n",
       "const           -58.9848      8.146     -7.241      0.000     -74.951     -43.018\n",
       "bathrooms         0.0362      0.003     10.478      0.000       0.029       0.043\n",
       "sqft_living       0.0002   3.47e-06     49.098      0.000       0.000       0.000\n",
       "sqft_lot       6.026e-07   5.17e-08     11.645      0.000    5.01e-07    7.04e-07\n",
       "floors           -0.0012      0.004     -0.323      0.746      -0.009       0.006\n",
       "waterfront        0.4896      0.020     25.008      0.000       0.451       0.528\n",
       "view              0.0545      0.002     23.597      0.000       0.050       0.059\n",
       "condition         0.0523      0.003     20.356      0.000       0.047       0.057\n",
       "grade             0.1001      0.002     41.884      0.000       0.095       0.105\n",
       "yr_renovated   3.853e-05   4.33e-06      8.895      0.000       3e-05     4.7e-05\n",
       "lat               0.5708      0.085      6.733      0.000       0.405       0.737\n",
       "long             -0.3514      0.060     -5.855      0.000      -0.469      -0.234\n",
       "sqft_living15  9.477e-05   3.88e-06     24.441      0.000    8.72e-05       0.000\n",
       "sqft_lot15     5.398e-08   8.18e-08      0.659      0.510   -1.06e-07    2.14e-07\n",
       "age               0.0007   8.56e-05      7.998      0.000       0.001       0.001\n",
       "x_98002          -0.0127      0.020     -0.642      0.521      -0.051       0.026\n",
       "x_98003          -0.0022      0.017     -0.128      0.898      -0.036       0.032\n",
       "x_98004           0.9006      0.031     28.600      0.000       0.839       0.962\n",
       "x_98005           0.5637      0.034     16.769      0.000       0.498       0.630\n",
       "x_98006           0.4908      0.028     17.777      0.000       0.437       0.545\n",
       "x_98007           0.5024      0.035     14.419      0.000       0.434       0.571\n",
       "x_98008           0.5063      0.033     15.445      0.000       0.442       0.571\n",
       "x_98010           0.3148      0.029     10.889      0.000       0.258       0.371\n",
       "x_98011           0.2054      0.043      4.790      0.000       0.121       0.289\n",
       "x_98014           0.2417      0.047      5.180      0.000       0.150       0.333\n",
       "x_98019           0.1826      0.046      3.946      0.000       0.092       0.273\n",
       "x_98022           0.1932      0.026      7.485      0.000       0.143       0.244\n",
       "x_98023          -0.0757      0.016     -4.766      0.000      -0.107      -0.045\n",
       "x_98024           0.4017      0.041      9.843      0.000       0.322       0.482\n",
       "x_98027           0.4306      0.028     15.411      0.000       0.376       0.485\n",
       "x_98028           0.1506      0.042      3.611      0.000       0.069       0.232\n",
       "x_98029           0.5300      0.032     16.553      0.000       0.467       0.593\n",
       "x_98030           0.0470      0.019      2.421      0.015       0.009       0.085\n",
       "x_98031           0.0383      0.020      1.938      0.053      -0.000       0.077\n",
       "x_98032          -0.0864      0.024     -3.659      0.000      -0.133      -0.040\n",
       "x_98033           0.5801      0.036     16.217      0.000       0.510       0.650\n",
       "x_98034           0.3084      0.038      8.057      0.000       0.233       0.383\n",
       "x_98038           0.2154      0.021     10.110      0.000       0.174       0.257\n",
       "x_98039           1.0385      0.042     24.803      0.000       0.956       1.121\n",
       "x_98040           0.6956      0.028     24.966      0.000       0.641       0.750\n",
       "x_98042           0.0777      0.018      4.267      0.000       0.042       0.113\n",
       "x_98045           0.4060      0.039     10.441      0.000       0.330       0.482\n",
       "x_98052           0.4583      0.036     12.607      0.000       0.387       0.530\n",
       "x_98053           0.4510      0.039     11.582      0.000       0.375       0.527\n",
       "x_98055           0.0617      0.022      2.825      0.005       0.019       0.105\n",
       "x_98056           0.2232      0.024      9.297      0.000       0.176       0.270\n",
       "x_98058           0.1095      0.021      5.241      0.000       0.069       0.150\n",
       "x_98059           0.2757      0.024     11.702      0.000       0.230       0.322\n",
       "x_98065           0.4066      0.036     11.292      0.000       0.336       0.477\n",
       "x_98070           0.2039      0.028      7.277      0.000       0.149       0.259\n",
       "x_98072           0.2707      0.043      6.359      0.000       0.187       0.354\n",
       "x_98074           0.4396      0.034     12.762      0.000       0.372       0.507\n",
       "x_98075           0.4619      0.033     14.008      0.000       0.397       0.527\n",
       "x_98077           0.2543      0.044      5.747      0.000       0.168       0.341\n",
       "x_98092           0.0537      0.018      3.061      0.002       0.019       0.088\n",
       "x_98102           0.6748      0.036     18.503      0.000       0.603       0.746\n",
       "x_98103           0.5292      0.035     15.300      0.000       0.461       0.597\n",
       "x_98105           0.6784      0.035     19.145      0.000       0.609       0.748\n",
       "x_98106           0.1394      0.026      5.439      0.000       0.089       0.190\n",
       "x_98107           0.5354      0.036     14.968      0.000       0.465       0.605\n",
       "x_98108           0.1619      0.028      5.776      0.000       0.107       0.217\n",
       "x_98109           0.7025      0.037     18.945      0.000       0.630       0.775\n",
       "x_98112           0.7919      0.033     24.361      0.000       0.728       0.856\n",
       "x_98115           0.5425      0.035     15.424      0.000       0.474       0.611\n",
       "x_98116           0.5213      0.029     18.192      0.000       0.465       0.577\n",
       "x_98117           0.5089      0.036     14.280      0.000       0.439       0.579\n",
       "x_98118           0.2772      0.025     11.051      0.000       0.228       0.326\n",
       "x_98119           0.7055      0.035     20.382      0.000       0.638       0.773\n",
       "x_98122           0.5581      0.031     18.006      0.000       0.497       0.619\n",
       "x_98125           0.2929      0.038      7.717      0.000       0.219       0.367\n",
       "x_98126           0.3181      0.026     12.019      0.000       0.266       0.370\n",
       "x_98133           0.1580      0.039      4.032      0.000       0.081       0.235\n",
       "x_98136           0.4633      0.027     17.111      0.000       0.410       0.516\n",
       "x_98144           0.4497      0.029     15.597      0.000       0.393       0.506\n",
       "x_98146           0.1122      0.024      4.614      0.000       0.065       0.160\n",
       "x_98148           0.0511      0.032      1.588      0.112      -0.012       0.114\n",
       "x_98155           0.1435      0.041      3.517      0.000       0.064       0.223\n",
       "x_98166           0.1803      0.022      8.200      0.000       0.137       0.223\n",
       "x_98168          -0.0601      0.023     -2.581      0.010      -0.106      -0.014\n",
       "x_98177           0.2821      0.041      6.861      0.000       0.201       0.363\n",
       "x_98178           0.0183      0.024      0.763      0.446      -0.029       0.065\n",
       "x_98188           0.0064      0.024      0.260      0.795      -0.042       0.054\n",
       "x_98198          -0.0338      0.019     -1.792      0.073      -0.071       0.003\n",
       "x_98199           0.5655      0.034     16.742      0.000       0.499       0.632\n",
       "==============================================================================\n",
       "Omnibus:                     1368.515   Durbin-Watson:                   2.015\n",
       "Prob(Omnibus):                  0.000   Jarque-Bera (JB):             6257.953\n",
       "Skew:                          -0.297   Prob(JB):                         0.00\n",
       "Kurtosis:                       5.987   Cond. No.                     2.77e+08\n",
       "==============================================================================\n",
       "\n",
       "Notes:\n",
       "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
       "[2] The condition number is large, 2.77e+08. This might indicate that there are\n",
       "strong multicollinearity or other numerical problems.\n",
       "\"\"\""
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = sm.add_constant(X_train)\n",
    "sm.OLS(y_train, X).fit().summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross Validate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Train score: 0.8755553043828836 : Standard Deviation: 0.0009434757652273278\n",
      "Mean Test score: 0.8737720639290373 : Standard Deviation: 0.003688083475487965\n"
     ]
    }
   ],
   "source": [
    "scores = cross_validate(\n",
    "                    lr, X_train, y_train, cv=5, \n",
    "                    return_train_score=True\n",
    ")\n",
    "mean_train_score = np.mean(scores['train_score'])\n",
    "train_score_std = np.std(scores['train_score'])\n",
    "mean_test_score = np.mean(scores['test_score'])\n",
    "test_score_std = np.std(scores['test_score'])\n",
    "\n",
    "print('Mean Train score:', mean_train_score, ': Standard Deviation:', train_score_std)\n",
    "print('Mean Test score:', mean_test_score, ': Standard Deviation:', test_score_std)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, you have been provided with 19 independent features.  You may use as many of them as you like in your model.  The goal is to get the highest R^2 on the test data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But how will you know that your model resulted in a high R^2 in the test data? You won't! At least, you won't know until the submission window has closed.  \n",
    "\n",
    "You will notice that while you have a file named `Xtest.csv`, you do not have a file named `ytest.csv`. Your instructor has that in their posession, and will keep it secret from the bakeoff contestants.  \n",
    "\n",
    "Once you have decided on your best model, you will then make predictions.  These predictions will be compared to the labels held in the hidden `ytest.csv`, resulting in a final R^2 score. In order for your submission to be valid, you have to have a prediction for every row of `Xtest.csv`.\n",
    "\n",
    "Below, the `Xtest.csv` has been imported into this notebook for you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bathrooms</th>\n",
       "      <th>sqft_living</th>\n",
       "      <th>sqft_lot</th>\n",
       "      <th>floors</th>\n",
       "      <th>waterfront</th>\n",
       "      <th>view</th>\n",
       "      <th>condition</th>\n",
       "      <th>grade</th>\n",
       "      <th>yr_renovated</th>\n",
       "      <th>lat</th>\n",
       "      <th>...</th>\n",
       "      <th>x_98146</th>\n",
       "      <th>x_98148</th>\n",
       "      <th>x_98155</th>\n",
       "      <th>x_98166</th>\n",
       "      <th>x_98168</th>\n",
       "      <th>x_98177</th>\n",
       "      <th>x_98178</th>\n",
       "      <th>x_98188</th>\n",
       "      <th>x_98198</th>\n",
       "      <th>x_98199</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.75</td>\n",
       "      <td>850</td>\n",
       "      <td>8573</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>0.0</td>\n",
       "      <td>47.5030</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.00</td>\n",
       "      <td>1510</td>\n",
       "      <td>6083</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>0.0</td>\n",
       "      <td>47.6966</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2.25</td>\n",
       "      <td>1790</td>\n",
       "      <td>42000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>0.0</td>\n",
       "      <td>47.4819</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.50</td>\n",
       "      <td>1140</td>\n",
       "      <td>2500</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>0.0</td>\n",
       "      <td>47.5707</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.00</td>\n",
       "      <td>1500</td>\n",
       "      <td>3920</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>0.0</td>\n",
       "      <td>47.6718</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 83 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   bathrooms  sqft_living  sqft_lot  floors  waterfront  view  condition  \\\n",
       "0       0.75          850      8573     1.0         0.0   0.0          3   \n",
       "1       1.00         1510      6083     1.0         0.0   0.0          4   \n",
       "2       2.25         1790     42000     1.0         0.0   0.0          3   \n",
       "3       1.50         1140      2500     1.0         0.0   1.0          3   \n",
       "4       1.00         1500      3920     1.0         0.0   0.0          3   \n",
       "\n",
       "   grade  yr_renovated      lat  ...  x_98146  x_98148  x_98155  x_98166  \\\n",
       "0      6           0.0  47.5030  ...        1        0        0        0   \n",
       "1      6           0.0  47.6966  ...        0        0        0        0   \n",
       "2      7           0.0  47.4819  ...        0        0        0        0   \n",
       "3      7           0.0  47.5707  ...        0        0        0        0   \n",
       "4      7           0.0  47.6718  ...        0        0        0        0   \n",
       "\n",
       "   x_98168  x_98177  x_98178  x_98188  x_98198  x_98199  \n",
       "0        0        0        0        0        0        0  \n",
       "1        0        0        0        0        0        0  \n",
       "2        0        0        0        0        0        0  \n",
       "3        0        0        0        0        0        0  \n",
       "4        0        0        0        0        0        0  \n",
       "\n",
       "[5 rows x 83 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "X_test.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5400, 83)\n"
     ]
    }
   ],
   "source": [
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice how the cell above indicates that there are **5400** records in `X_test`.  You should therefore submit 5400 predicted saleprices.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building Your Best Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So how does one build a model that one has confidence will perform well on the test data? You could just fit the model on the training data, and consider the R^2.  But remember, no matter what, your training R^2  will always go up when you add more features. With that in mind, you could just implement a 6th degree polynomial transformation, and your training R^2 will be very high.  What will that mean in terms of the bias-variance tradeoff?  Your model will be highly complex and surely overfit. Therefore, you would expect it to perform poorly on the test set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get an idea of how your model will perform on unseen data, you will have to choose some method of creating a validation set within your training data.  \n",
    "\n",
    "There are several ways to do that, and you will have to pick the method that you are most comfortable with.  \n",
    "\n",
    "The simplest way would be to simply perform another train-test-split on your training data, fit your model on the larger part of that secondary split, and then score your model on the smaller validation set. \n",
    "\n",
    "The more comprehensive way would be to use the Sklearn cross-validation class or Kfolds.  If you specify 5 folds, then you train your model on 5 different sets of training data and 5 different sets of validation data.  You would then look at the mean R^2 of the 5 validation sets.\n",
    "\n",
    "Your task will be to try out different hypotheses iteratively, and select the combination of predictors that explains the most variance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generating Predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After you have selected your best combination of features, your work is not quite done. You have to use your trained model to make predictions.  In doing so, you have to watch out for a few stumbling points."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1: Retrain Your Model on the Entire Training Set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When you are iteratively building your model with cross-validation, you are required to leave out some data (the validation data) in the training process.  You always want to train your model on as much data as possible. The validation process tells you which features to use in your final model, but you need to then retrain your model on the entire training data using those features.  You could not perform this step, but your model will perform worse. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2: Prepare your X_test Exactly as You Prepared your X_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When selecting the best features for your model, you will most certainly alter your X_train data frame.  For example, maybe you did not include the `date` feature. After fitting your final model to a version of X_train without date, you then try to make a prediction on X_test.  Sklearn will complain that the dimensions of X_test do not match the demensions required on the fit model.  So, before making your predictions, you will have to drop the `date` column from X_test.  Any transformation you do to X_train will have to be performed on X_test. \n",
    "\n",
    "You will also have to deal with the missing values in the X_test.  There are 3 columns which include NA's.  You will not be able to drop rows containing missing values, since doing so will result in diminishing the number of predictions in your final set. If those columns are important to your model, you will have to fill the NA's in the test set just as you did in your training set. Of course, you could opt to not include those columns in your final model.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Checking your Prediction Shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You have selected the features for your best model, and trained your model on the entire data set.  You have transformed the X_test in the same way that you transformed your X_train.  You have made a set of predictions. \n",
    "\n",
    "In the cell below, you will find a fake y_test; it has been filled with zeros."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "y_test_fake = np.full((5400,1), 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to test that your predictions are of the correct shape, feed your 5400 predicted values into the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(530613.0565927846, (5400, 1))"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_pred = np.exp(pred_lr)\n",
    "final_pred.mean(), final_pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1.7285104818119206"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import r2_score\n",
    "\n",
    "# fake predictions using the mean of y_train.\n",
    "your_y_hat_predictions = np.full((5400,1), np.mean(y_train))\n",
    "\n",
    "r2_score(final_pred, y_test_fake)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "530613.0565927846"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_pred.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Only pay attention to errors thrown by the cell above, not the R^2.   If the cell does not throw any errors, your predictions are ready for submission.\n",
    "\n",
    "Convert the array of predictions into a `csv` by filling in the placeholder filepath and variable name with the appropriate values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt('Group_4.csv', final_pred, delimiter=',')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There will be a Slack channel designated for submitting your final predictions `csv`. \n",
    "\n",
    "Only predictions received before 5 pm PST will be considered valid.  \n",
    "\n",
    "The team with the highest R^2 will be deemed the Linear Regression Bakeoff winner.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![on you marks, get set, bake](https://media.giphy.com/media/l3vRhl6k5tb3oPGLK/giphy.gif)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (learn-env)",
   "language": "python",
   "name": "learn-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
